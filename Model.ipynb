{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,SpatialDropout1D,Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_name):\n",
    "    data_list  = []\n",
    "    with open(file_name,'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label = ' '.join(line[:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data_list.append([label, text])\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(text_list):\n",
    "    label_list = []\n",
    "    text_list = [text_list[i][0].replace('[','') for i in range(len(text_list))]\n",
    "    label_list = [list(np.fromstring(text_list[i], dtype=float, sep=' ')) for i in range(len(text_list))]\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_msgs(text_list):\n",
    "    msg_list = []\n",
    "    msg_list = [text_list[i][1] for i in range(len(text_list))]\n",
    "    return msg_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_file):\n",
    "    with open(glove_file,'r',encoding='UTF-8') as file:\n",
    "        words = set()\n",
    "        word_to_vec = {}\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            line[0] = re.sub('[^a-zA-Z]', '', line[0])\n",
    "            if len(line[0]) > 0:\n",
    "                words.add(line[0])\n",
    "                try:\n",
    "                    word_to_vec[line[0]] = np.array(line[1:],dtype=np.float64)\n",
    "                except:\n",
    "                    print('Error has occured')\n",
    "                    print('-'*50)\n",
    "                    print(line[1:])\n",
    "        i = 1\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        for word in sorted(words):\n",
    "            word_to_index[word] = i\n",
    "            index_to_word[i] = word\n",
    "            i = i+1\n",
    "    #print(word_to_index,index_to_word,word_to_vec)\n",
    "    return word_to_index,index_to_word,word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(text_arr,word_to_index,max_len):\n",
    "    arr_len = text_arr.shape[0]\n",
    "    arr_indices = np.zeros((arr_len,max_len))\n",
    "    for i in range(arr_len):\n",
    "        sentence = text_arr[i].lower().split()\n",
    "        j = 0\n",
    "        for word in sentence:\n",
    "            if word in word_to_index:\n",
    "                arr_indices[i,j] = word_to_index[word]\n",
    "                j = j+1\n",
    "    return arr_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index,word_to_vec):\n",
    "    corpus_len = len(word_to_index) + 1\n",
    "    #print( \"w2v  => \" , word_to_vec)\n",
    "    embed_dim = word_to_vec['word'].shape[0]\n",
    "    \n",
    "    embed_matrix = np.zeros((corpus_len,embed_dim))\n",
    "\n",
    "    for word, index in word_to_index.items():\n",
    "        embed_matrix[index,:] = word_to_vec[word]\n",
    "\n",
    "    embedding_layer = Embedding(corpus_len, embed_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([embed_matrix])\n",
    "\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape,embedding_layer):\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    embedding_layer =  embedding_layer\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    reg = L1L2(0.01, 0.01)\n",
    "    \n",
    "    X = Bidirectional(LSTM(128, return_sequences=True,bias_regularizer=reg,kernel_initializer='he_uniform'))(embeddings)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(64)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(7, activation='softmax')(X)\n",
    "    X =  Activation('softmax')(X)\n",
    "    model = Model(sentence_indices, X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist = read_text_file(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = extract_labels(textlist)\n",
    "msg_list = extract_text_msgs(textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index,index_to_word,word_to_vec = read_glove_vector('../glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(msg_list, label_list,stratify = label_list,test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(msg_list)\n",
    "train_tokenized = tk.texts_to_sequences(x_train)\n",
    "test_tokenized = tk.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(train_tokenized, maxlen = maxlen)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('tokenizer.pickle'):\n",
    "    os.remove('tokenizer.pickle')\n",
    "    with open('tokenizer.pickle', 'wb') as tokenizer:\n",
    "        pickle.dump(tk, tokenizer, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 50)            17090100  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           183296    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 256)           1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 17,357,051\n",
      "Trainable params: 17,356,539\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = create_embedding_layer(word_to_index,word_to_vec)\n",
    "model = create_lstm_model((maxlen,),embedding_layer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shubham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/30\n",
      "5984/5984 [==============================] - 75s 12ms/step - loss: 6.3894 - acc: 0.1673\n",
      "Epoch 2/30\n",
      "5984/5984 [==============================] - 73s 12ms/step - loss: 5.1569 - acc: 0.2390\n",
      "Epoch 3/30\n",
      "5984/5984 [==============================] - 83s 14ms/step - loss: 4.1341 - acc: 0.3120\n",
      "Epoch 4/30\n",
      "5984/5984 [==============================] - 82s 14ms/step - loss: 3.3063 - acc: 0.3702\n",
      "Epoch 5/30\n",
      "5984/5984 [==============================] - 82s 14ms/step - loss: 2.6407 - acc: 0.4313\n",
      "Epoch 6/30\n",
      "5984/5984 [==============================] - 84s 14ms/step - loss: 2.1192 - acc: 0.4871\n",
      "Epoch 7/30\n",
      "5984/5984 [==============================] - 82s 14ms/step - loss: 1.7359 - acc: 0.5279\n",
      "Epoch 8/30\n",
      "5984/5984 [==============================] - 85s 14ms/step - loss: 1.6045 - acc: 0.5702\n",
      "Epoch 9/30\n",
      "5984/5984 [==============================] - 82s 14ms/step - loss: 1.5624 - acc: 0.6110\n",
      "Epoch 10/30\n",
      "5984/5984 [==============================] - 81s 14ms/step - loss: 1.5378 - acc: 0.6347\n",
      "Epoch 11/30\n",
      "5984/5984 [==============================] - 80s 13ms/step - loss: 1.5066 - acc: 0.6636\n",
      "Epoch 12/30\n",
      "5984/5984 [==============================] - 80s 13ms/step - loss: 1.4815 - acc: 0.6928\n",
      "Epoch 13/30\n",
      "5984/5984 [==============================] - 81s 13ms/step - loss: 1.4636 - acc: 0.7096\n",
      "Epoch 14/30\n",
      "5984/5984 [==============================] - 80s 13ms/step - loss: 1.4459 - acc: 0.7243\n",
      "Epoch 15/30\n",
      "5984/5984 [==============================] - 81s 13ms/step - loss: 1.4319 - acc: 0.7400\n",
      "Epoch 16/30\n",
      "5984/5984 [==============================] - 80s 13ms/step - loss: 1.4190 - acc: 0.7537\n",
      "Epoch 17/30\n",
      "5984/5984 [==============================] - 80s 13ms/step - loss: 1.3943 - acc: 0.7779\n",
      "Epoch 18/30\n",
      "5984/5984 [==============================] - 81s 14ms/step - loss: 1.3890 - acc: 0.7824\n",
      "Epoch 19/30\n",
      "5984/5984 [==============================] - 82s 14ms/step - loss: 1.3753 - acc: 0.7963\n",
      "Epoch 20/30\n",
      "5984/5984 [==============================] - 77s 13ms/step - loss: 1.3671 - acc: 0.8041\n",
      "Epoch 21/30\n",
      "5984/5984 [==============================] - 78s 13ms/step - loss: 1.3718 - acc: 0.8000\n",
      "Epoch 22/30\n",
      "5984/5984 [==============================] - 77s 13ms/step - loss: 1.3527 - acc: 0.8175\n",
      "Epoch 23/30\n",
      "5984/5984 [==============================] - 77s 13ms/step - loss: 1.3537 - acc: 0.8157\n",
      "Epoch 24/30\n",
      "5984/5984 [==============================] - 75s 13ms/step - loss: 1.3567 - acc: 0.8140\n",
      "Epoch 25/30\n",
      "5984/5984 [==============================] - 75s 13ms/step - loss: 1.3365 - acc: 0.8314\n",
      "Epoch 26/30\n",
      "5984/5984 [==============================] - 76s 13ms/step - loss: 1.3258 - acc: 0.8439\n",
      "Epoch 27/30\n",
      "5984/5984 [==============================] - 76s 13ms/step - loss: 1.3246 - acc: 0.8456\n",
      "Epoch 28/30\n",
      "5984/5984 [==============================] - 76s 13ms/step - loss: 1.3252 - acc: 0.8453\n",
      "Epoch 29/30\n",
      "5984/5984 [==============================] - 76s 13ms/step - loss: 1.3159 - acc: 0.8541\n",
      "Epoch 30/30\n",
      "5984/5984 [==============================] - 76s 13ms/step - loss: 1.3249 - acc: 0.8469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x281c8a7ac08>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, np.array(y_train), epochs = 30, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emoji_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496/1496 [==============================] - 6s 4ms/step\n",
      "1.680861423997318 0.48462566844919786\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, np.array(y_test))\n",
    "print(loss ,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "test_sent = tk.texts_to_sequences(['Feeling sad that my favourite cricketer has retired'])\n",
    "test_sent = pad_sequences(test_sent, maxlen = maxlen)\n",
    "pred = model.predict(test_sent)\n",
    "print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('emoji_model.h5' , compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "test_sent = tk.texts_to_sequences(['fuck you'])\n",
    "test_sent = pad_sequences(test_sent, maxlen = maxlen)\n",
    "pred = model.predict(test_sent)\n",
    "print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
