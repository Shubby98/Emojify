{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"DeepMoji-master/data/PsychExp/raw.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(file,'rb'),encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'texts', 'val_ind', 'test_ind', 'train_ind'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_index = data['val_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = data['test_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = data['train_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4352"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': array([0., 0., 1., 0., 0., 0., 0.])} When I saw a car run over a child, out of carelessness.\n"
     ]
    }
   ],
   "source": [
    "print(label[4352] , texts[4352])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vec(path):\n",
    "    import numpy as np\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_vec = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            key = line[0]\n",
    "            words.add(key)\n",
    "            val = np.array(line[1:],dtype=np.float64)\n",
    "            word_vec[key] = val\n",
    "            \n",
    "        # Dict (Vocab) --> set\n",
    "        word2idx = {}\n",
    "        idx2word = {}\n",
    "        \n",
    "        for i,w in enumerate(sorted(words)):\n",
    "            word2idx[w] = i\n",
    "            idx2word[i] = w\n",
    "            \n",
    "    return word_vec,word2idx,idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec,word2idx,idx2word = glove_vec('glove.6B/glove.6B.50d.txt')\n",
    "vocab_size = len(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY_data(Index):\n",
    "    X = np.zeros((len(Index) , 128 ))\n",
    "    Y = np.zeros((len(Index) , 7))\n",
    "    \n",
    "    zer = np.zeros((50 ,))\n",
    "    for i ,index in enumerate(Index):\n",
    "        line = word_tokenize(texts[index].lower())\n",
    "        \n",
    "        #print(index)\n",
    "        Y[i] = label[index]['label']\n",
    "        for j , word  in  enumerate(line[:128]):\n",
    "             X[i , j] =  word2idx.get(word , 0)\n",
    "    \n",
    "    return X ,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 128) (900, 7)\n",
      "(6480, 128) (6480, 7)\n",
      "(100, 128) (100, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train , Y_train  = get_XY_data(train_index)\n",
    "print(X_train.shape , Y_train.shape)\n",
    "\n",
    "X_test , Y_test  = get_XY_data(test_index)\n",
    "print(X_test.shape , Y_test.shape)\n",
    "\n",
    "X_val , Y_val  = get_XY_data(val_index)\n",
    "print(X_val.shape , Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding , Flatten ,Dense , Bidirectional ,LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 128, 50)           20000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 20,185,095\n",
      "Trainable params: 20,185,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=128))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 128) for input Tensor(\"embedding_10_input:0\", shape=(None, 128), dtype=float32), but it was called on an input with incompatible shape (32, 1).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.14201638, 0.14288639, 0.14181702, 0.14222808, 0.14324875,\n",
       "        0.14416346, 0.14363983],\n",
       "       [0.14374709, 0.14279792, 0.14387421, 0.14279208, 0.14169517,\n",
       "        0.14244094, 0.14265256],\n",
       "       [0.14311263, 0.14213657, 0.14232287, 0.14232603, 0.1428991 ,\n",
       "        0.14273866, 0.14446412],\n",
       "       [0.14223164, 0.14289026, 0.1431509 , 0.14364623, 0.1427494 ,\n",
       "        0.14296745, 0.1423641 ],\n",
       "       [0.14202522, 0.14329107, 0.14250772, 0.14231656, 0.14338285,\n",
       "        0.14421962, 0.14225693],\n",
       "       [0.14295256, 0.1418416 , 0.14305754, 0.14280866, 0.1426298 ,\n",
       "        0.14314488, 0.14356494],\n",
       "       [0.14336945, 0.1429956 , 0.14165287, 0.14225993, 0.14276905,\n",
       "        0.14412022, 0.1428329 ],\n",
       "       [0.1434992 , 0.14325027, 0.14301878, 0.14189154, 0.14319955,\n",
       "        0.14272243, 0.14241822],\n",
       "       [0.14223164, 0.14289026, 0.1431509 , 0.14364623, 0.1427494 ,\n",
       "        0.14296745, 0.1423641 ],\n",
       "       [0.14414805, 0.14239877, 0.1423076 , 0.14307176, 0.1418484 ,\n",
       "        0.14324445, 0.14298101],\n",
       "       [0.14199007, 0.14285916, 0.14383785, 0.14360799, 0.14265162,\n",
       "        0.14167397, 0.14337938],\n",
       "       [0.14368942, 0.143169  , 0.14206952, 0.14264598, 0.14312205,\n",
       "        0.1425632 , 0.14274079],\n",
       "       [0.1420622 , 0.14189835, 0.14315203, 0.14312114, 0.14303471,\n",
       "        0.14298232, 0.14374934],\n",
       "       [0.14327195, 0.14317963, 0.14296424, 0.14201185, 0.14258015,\n",
       "        0.14299478, 0.1429973 ],\n",
       "       [0.14374709, 0.14279792, 0.14387421, 0.14279208, 0.14169517,\n",
       "        0.14244094, 0.14265256],\n",
       "       [0.1429594 , 0.14360313, 0.14252837, 0.14329925, 0.14326441,\n",
       "        0.14272004, 0.14162527],\n",
       "       [0.14259146, 0.1439881 , 0.14264217, 0.14347026, 0.14211255,\n",
       "        0.1419417 , 0.1432537 ],\n",
       "       [0.14199007, 0.14285916, 0.14383785, 0.14360799, 0.14265162,\n",
       "        0.14167397, 0.14337938],\n",
       "       [0.1433345 , 0.14252186, 0.14324462, 0.14254913, 0.1426733 ,\n",
       "        0.1422182 , 0.14345844],\n",
       "       [0.14338322, 0.143581  , 0.1422814 , 0.14325547, 0.14238985,\n",
       "        0.1420189 , 0.1430902 ],\n",
       "       [0.14208645, 0.14358072, 0.14290497, 0.14283562, 0.14296694,\n",
       "        0.14332086, 0.14230454],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246],\n",
       "       [0.14342673, 0.14273718, 0.14281598, 0.14304051, 0.14324878,\n",
       "        0.14201833, 0.14271246]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 385ms/step - loss: 1.6119 - accuracy: 0.3789 - val_loss: 1.8974 - val_accuracy: 0.2100\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 4s 417ms/step - loss: 1.5033 - accuracy: 0.4156 - val_loss: 1.8752 - val_accuracy: 0.2400\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 4s 412ms/step - loss: 1.3803 - accuracy: 0.5111 - val_loss: 1.8596 - val_accuracy: 0.3000\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 4s 432ms/step - loss: 1.6879 - accuracy: 0.5556 - val_loss: 2.0362 - val_accuracy: 0.2700\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 4s 405ms/step - loss: 1.3620 - accuracy: 0.5200 - val_loss: 2.0154 - val_accuracy: 0.2700\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 4s 428ms/step - loss: 1.1992 - accuracy: 0.5544 - val_loss: 2.0059 - val_accuracy: 0.2700\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 4s 407ms/step - loss: 1.0304 - accuracy: 0.6922 - val_loss: 1.9561 - val_accuracy: 0.3000\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 4s 404ms/step - loss: 0.8875 - accuracy: 0.7678 - val_loss: 2.0541 - val_accuracy: 0.3400\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 4s 434ms/step - loss: 0.7632 - accuracy: 0.8078 - val_loss: 1.9953 - val_accuracy: 0.3700\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 4s 405ms/step - loss: 0.6451 - accuracy: 0.8422 - val_loss: 2.1146 - val_accuracy: 0.3600\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 4s 433ms/step - loss: 0.5400 - accuracy: 0.8744 - val_loss: 2.3052 - val_accuracy: 0.3100\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 4s 403ms/step - loss: 0.4586 - accuracy: 0.8911 - val_loss: 2.2721 - val_accuracy: 0.3500\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 4s 416ms/step - loss: 0.5202 - accuracy: 0.8578 - val_loss: 2.1379 - val_accuracy: 0.2900\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 4s 407ms/step - loss: 0.4900 - accuracy: 0.9033 - val_loss: 2.3502 - val_accuracy: 0.3700\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 4s 406ms/step - loss: 0.3712 - accuracy: 0.9244 - val_loss: 2.2974 - val_accuracy: 0.3800\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 4s 431ms/step - loss: 0.2987 - accuracy: 0.9511 - val_loss: 2.4848 - val_accuracy: 0.3500\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 4s 405ms/step - loss: 0.3122 - accuracy: 0.9533 - val_loss: 2.4908 - val_accuracy: 0.4100\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 4s 429ms/step - loss: 0.2582 - accuracy: 0.9644 - val_loss: 2.6640 - val_accuracy: 0.3700\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 4s 403ms/step - loss: 0.2122 - accuracy: 0.9733 - val_loss: 2.7633 - val_accuracy: 0.3800\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 4s 401ms/step - loss: 0.1769 - accuracy: 0.9756 - val_loss: 2.9946 - val_accuracy: 0.3100\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 4s 423ms/step - loss: 0.1524 - accuracy: 0.9778 - val_loss: 2.9960 - val_accuracy: 0.3200\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 4s 409ms/step - loss: 0.1311 - accuracy: 0.9789 - val_loss: 3.1362 - val_accuracy: 0.3200\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 4s 434ms/step - loss: 0.1104 - accuracy: 0.9844 - val_loss: 3.1256 - val_accuracy: 0.3000\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 4s 417ms/step - loss: 0.0975 - accuracy: 0.9867 - val_loss: 3.3817 - val_accuracy: 0.2800\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 4s 483ms/step - loss: 0.0883 - accuracy: 0.9867 - val_loss: 3.3724 - val_accuracy: 0.3000\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 4s 403ms/step - loss: 0.0824 - accuracy: 0.9900 - val_loss: 3.4349 - val_accuracy: 0.3200\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 4s 403ms/step - loss: 0.0724 - accuracy: 0.9911 - val_loss: 3.5285 - val_accuracy: 0.3500\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 4s 418ms/step - loss: 0.0656 - accuracy: 0.9933 - val_loss: 3.6799 - val_accuracy: 0.3100\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 4s 409ms/step - loss: 0.0595 - accuracy: 0.9911 - val_loss: 3.8132 - val_accuracy: 0.2900\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 4s 434ms/step - loss: 0.0575 - accuracy: 0.9922 - val_loss: 3.7776 - val_accuracy: 0.3100\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 4s 405ms/step - loss: 0.0548 - accuracy: 0.9933 - val_loss: 3.6143 - val_accuracy: 0.3200\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 4s 429ms/step - loss: 0.0526 - accuracy: 0.9933 - val_loss: 3.8575 - val_accuracy: 0.2800\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 4s 403ms/step - loss: 0.0519 - accuracy: 0.9944 - val_loss: 3.9105 - val_accuracy: 0.3200\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 4s 407ms/step - loss: 0.0424 - accuracy: 0.9978 - val_loss: 3.8163 - val_accuracy: 0.3000\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 4s 433ms/step - loss: 0.0386 - accuracy: 0.9967 - val_loss: 3.8616 - val_accuracy: 0.3100\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 4s 411ms/step - loss: 0.0353 - accuracy: 0.9967 - val_loss: 3.9055 - val_accuracy: 0.3000\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 4s 434ms/step - loss: 0.0317 - accuracy: 0.9978 - val_loss: 4.0115 - val_accuracy: 0.3100\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 4s 408ms/step - loss: 0.0281 - accuracy: 0.9978 - val_loss: 4.0040 - val_accuracy: 0.3300\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 4s 424ms/step - loss: 0.0249 - accuracy: 0.9978 - val_loss: 4.0677 - val_accuracy: 0.3300\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 4s 412ms/step - loss: 0.0226 - accuracy: 0.9989 - val_loss: 4.1733 - val_accuracy: 0.3200\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 4s 411ms/step - loss: 0.0210 - accuracy: 0.9978 - val_loss: 4.1451 - val_accuracy: 0.3300\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 4s 433ms/step - loss: 0.0207 - accuracy: 0.9978 - val_loss: 4.1849 - val_accuracy: 0.3400\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 4s 410ms/step - loss: 0.0233 - accuracy: 0.9967 - val_loss: 4.1547 - val_accuracy: 0.3300\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 4s 434ms/step - loss: 0.0438 - accuracy: 0.9944 - val_loss: 4.1098 - val_accuracy: 0.3600\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 4s 406ms/step - loss: 0.0310 - accuracy: 0.9978 - val_loss: 3.9639 - val_accuracy: 0.3500\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 4s 417ms/step - loss: 0.0487 - accuracy: 0.9878 - val_loss: 4.1121 - val_accuracy: 0.3200\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 4s 413ms/step - loss: 0.0293 - accuracy: 0.9989 - val_loss: 3.7616 - val_accuracy: 0.3200\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 4s 407ms/step - loss: 0.0692 - accuracy: 0.9867 - val_loss: 4.0186 - val_accuracy: 0.3000\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 4s 434ms/step - loss: 0.0496 - accuracy: 0.9922 - val_loss: 3.7135 - val_accuracy: 0.3500\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 4s 406ms/step - loss: 0.0491 - accuracy: 0.9922 - val_loss: 3.8081 - val_accuracy: 0.3500\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 4s 431ms/step - loss: 0.0387 - accuracy: 0.9944 - val_loss: 3.7641 - val_accuracy: 0.3300\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 4s 407ms/step - loss: 0.0599 - accuracy: 0.9889 - val_loss: 3.7557 - val_accuracy: 0.3600\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 4s 438ms/step - loss: 0.0564 - accuracy: 0.9900 - val_loss: 3.6231 - val_accuracy: 0.3100\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 4s 421ms/step - loss: 0.0494 - accuracy: 0.9922 - val_loss: 3.6195 - val_accuracy: 0.3200\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 4s 406ms/step - loss: 0.0251 - accuracy: 0.9989 - val_loss: 3.7956 - val_accuracy: 0.3500\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 4s 433ms/step - loss: 0.0209 - accuracy: 0.9989 - val_loss: 3.9243 - val_accuracy: 0.3600\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 4s 407ms/step - loss: 0.0169 - accuracy: 0.9989 - val_loss: 4.0271 - val_accuracy: 0.3700\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 4s 439ms/step - loss: 0.0146 - accuracy: 0.9989 - val_loss: 4.1452 - val_accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "9/9 [==============================] - 4s 402ms/step - loss: 0.0130 - accuracy: 0.9978 - val_loss: 4.1681 - val_accuracy: 0.3600\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 4s 402ms/step - loss: 0.0118 - accuracy: 0.9989 - val_loss: 4.1950 - val_accuracy: 0.3500\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 4s 421ms/step - loss: 0.0110 - accuracy: 0.9989 - val_loss: 4.2783 - val_accuracy: 0.3400\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 4s 405ms/step - loss: 0.0102 - accuracy: 0.9989 - val_loss: 4.3231 - val_accuracy: 0.3400\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 4s 428ms/step - loss: 0.0097 - accuracy: 0.9978 - val_loss: 4.3699 - val_accuracy: 0.3400\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 4s 401ms/step - loss: 0.0092 - accuracy: 0.9989 - val_loss: 4.4168 - val_accuracy: 0.3300\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 4s 428ms/step - loss: 0.0086 - accuracy: 0.9989 - val_loss: 4.4684 - val_accuracy: 0.3300\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 4s 401ms/step - loss: 0.0082 - accuracy: 0.9989 - val_loss: 4.4954 - val_accuracy: 0.3300\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 4s 416ms/step - loss: 0.0083 - accuracy: 0.9989 - val_loss: 4.5048 - val_accuracy: 0.3300\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 4s 429ms/step - loss: 0.0076 - accuracy: 0.9989 - val_loss: 4.5355 - val_accuracy: 0.3300\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 4s 415ms/step - loss: 0.0073 - accuracy: 0.9989 - val_loss: 4.5839 - val_accuracy: 0.3400\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 4s 430ms/step - loss: 0.0074 - accuracy: 0.9989 - val_loss: 4.6264 - val_accuracy: 0.3400\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 4s 404ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 4.6794 - val_accuracy: 0.3300\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 4s 417ms/step - loss: 0.0068 - accuracy: 0.9989 - val_loss: 4.6703 - val_accuracy: 0.3300\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 4s 406ms/step - loss: 0.0065 - accuracy: 0.9989 - val_loss: 4.6761 - val_accuracy: 0.3400\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 4s 407ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 4.7492 - val_accuracy: 0.3500\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 4s 430ms/step - loss: 0.0065 - accuracy: 0.9989 - val_loss: 4.7385 - val_accuracy: 0.3200\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 4s 402ms/step - loss: 0.0070 - accuracy: 0.9989 - val_loss: 4.6650 - val_accuracy: 0.3400\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 4s 444ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 4.7915 - val_accuracy: 0.3500\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 4s 410ms/step - loss: 0.0057 - accuracy: 0.9978 - val_loss: 4.8359 - val_accuracy: 0.3400\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 4s 416ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 4.8661 - val_accuracy: 0.3500\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 4s 429ms/step - loss: 0.0059 - accuracy: 0.9978 - val_loss: 4.8226 - val_accuracy: 0.3400\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 4s 412ms/step - loss: 0.0057 - accuracy: 0.9978 - val_loss: 4.8590 - val_accuracy: 0.3400\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 0.0052 - accuracy: 0.9978 - val_loss: 4.8714 - val_accuracy: 0.3500\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 4s 483ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 4.8757 - val_accuracy: 0.3400\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 4s 449ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 4.9278 - val_accuracy: 0.3400\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 5s 530ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 4.9685 - val_accuracy: 0.3300\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 4s 449ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 4.8313 - val_accuracy: 0.3600\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 4s 419ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 4.9700 - val_accuracy: 0.3200\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 4s 453ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 4.9835 - val_accuracy: 0.3400\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 4s 415ms/step - loss: 0.0046 - accuracy: 0.9978 - val_loss: 5.0119 - val_accuracy: 0.3300\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 4s 435ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 4.9669 - val_accuracy: 0.3400\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 4s 414ms/step - loss: 0.0043 - accuracy: 0.9978 - val_loss: 5.0513 - val_accuracy: 0.3500\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 4s 422ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 5.0593 - val_accuracy: 0.3400\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 4s 446ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 4.9999 - val_accuracy: 0.3400\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 4s 417ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 5.1502 - val_accuracy: 0.3400\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 4s 450ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 5.1393 - val_accuracy: 0.3400\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 4s 416ms/step - loss: 0.0046 - accuracy: 0.9978 - val_loss: 4.9860 - val_accuracy: 0.3400\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 4s 440ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 5.0815 - val_accuracy: 0.3400\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 4s 416ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 5.1518 - val_accuracy: 0.3500\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 4s 419ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 4.7981 - val_accuracy: 0.3700\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 4s 442ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 4.8128 - val_accuracy: 0.3500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train , Y_train , batch_size = 100 , epochs=100 , validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 9s 42ms/step - loss: 4.7417 - accuracy: 0.3020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.741705417633057, 0.30200618505477905]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
